{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosch Kaggle Script\n",
    "## To Do:\n",
    "* Objectify\n",
    "* LabelEncode Data?\n",
    "* Decision Tree\n",
    "* K-cross validation (going to be kind of tricky) Could just do within batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "from collections import defaultdict\n",
    "import cPickle\n",
    "from sklearn import svm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "# Chunksize that seems to work for 32GB of RAM\n",
    "# Possible modify this\n",
    "# chunksize = 50000\n",
    "chunksize = 1000\n",
    "\n",
    "file_names = {}\n",
    "file_names['train'] = {}\n",
    "file_names['test'] = {}\n",
    "\n",
    "# file_names['train']['date'] = 'data/train_date.csv'\n",
    "file_names['train']['categorical'] = 'data/train_categorical.csv'\n",
    "file_names['train']['numeric'] = 'data/train_numeric.csv'\n",
    "# file_names['test']['date'] = 'data/test_date.csv'\n",
    "file_names['test']['categorical'] = 'data/test_categorical.csv'\n",
    "file_names['test']['numeric'] = 'data/test_numeric.csv'\n",
    "\n",
    "model_dir = 'data/models/'\n",
    "sub_dir = 'data/submissions/'\n",
    "\n",
    "# This is for the categories and date\n",
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "\n",
    "# XGBoost stuff\n",
    "num_rounds = 2\n",
    "params = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_save_models():\n",
    "    # This is what will be returned\n",
    "#     nlines = subprocess.check_output('wc -l %s' % file_names['train']['numeric'], shell=True)\n",
    "#     nlines = int(nlines.split()[0])\n",
    "    nlines = 1000\n",
    "    train_types = {}\n",
    "    train_columns = {}\n",
    "    \n",
    "    for file_name in file_names['train']:\n",
    "        train_data = pd.read_csv(file_names['train'][file_name], nrows=2)\n",
    "        train_types[file_name] = train_data.dtypes\n",
    "        train_columns[file_name] = train_data.columns\n",
    "    \n",
    "    time_start = time.time()\n",
    "    total_start = time_start\n",
    "    \n",
    "    total_sum = {}\n",
    "    \n",
    "    total_sum[0] = 0\n",
    "    total_sum[1] = 0\n",
    "    \n",
    "    for i in range(1, nlines, chunksize):\n",
    "        print('training on lines: %d - %d' % (i-1, i+chunksize-1))\n",
    "        train_data_parts = {}\n",
    "        for file_name in file_names['train']:\n",
    "            train_data_parts[file_name] = pd.read_csv(\n",
    "                file_names['train'][file_name],\n",
    "                index_col=0,\n",
    "                header=None,\n",
    "                names=train_columns[file_name],\n",
    "                nrows=chunksize,\n",
    "                skiprows=i,\n",
    "                dtype=train_types[file_name]\n",
    "                )\n",
    "        \n",
    "        # Let's normalize or whatever\n",
    "        train_data = train_data_parts['numeric'].join(train_data_parts['categorical']).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "        train_labels = train_data['Response']\n",
    "        train_data.drop('Response', axis=1, inplace=True)\n",
    "    \n",
    "        classifier = svm.SVC()\n",
    "        \n",
    "        print('Training')\n",
    "        classifier.fit(train_data, train_labels)\n",
    "        print('Time to train: %s seconds' % (time.time() - time_start))\n",
    "        \n",
    "        # Save model\n",
    "        with open('data/models/svm.' + str(i) + '.pkl', 'wb') as fid:\n",
    "            cPickle.dump(classifier, fid)\n",
    "#         d_train = xgb.DMatrix(train_data_df.as_matrix(), label=train_data_df['Response'])\n",
    "        \n",
    "        # run this shit\n",
    "#         bst = xgb.cv(params, d_train, num_rounds, nfold=2, verbose_eval=True)\n",
    "#         bst.save_model('data/models/test_model.' + str(i) + '.model')\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_and_make_submission():    \n",
    "    # all of the line numbers are the same (I hope or we have bigger issues)\n",
    "#     nlines = subprocess.check_output('wc -l %s' % file_names['test']['numeric'], shell=True)\n",
    "#     nlines = int(nlines.split()[0])\n",
    "    nlines = 1000\n",
    "    test_types = {}\n",
    "    test_columns = {}\n",
    "    \n",
    "    for file_name in file_names['test']:\n",
    "        test_data = pd.read_csv(file_names['test'][file_name], nrows=2)\n",
    "        test_types[file_name] = test_data.dtypes\n",
    "        test_columns[file_name] = test_data.columns\n",
    "    \n",
    "    # Eventually have this use ensemble\n",
    "    with open('data/models/svm.1.pkl', 'rb') as fid:\n",
    "        classifier = cPickle.load(fid)\n",
    "    \n",
    "    for i in range(1, nlines, chunksize):\n",
    "        print('testing on lines: %d - %d' % (i-1, i+chunksize-1))\n",
    "        test_data_parts = {}\n",
    "        for file_name in file_names['test']:\n",
    "            test_data_parts[file_name] = pd.read_csv(\n",
    "                    file_names['test'][file_name],\n",
    "                    index_col=0,\n",
    "                    header=None,\n",
    "                    names=test_columns[file_name],\n",
    "                    nrows=chunksize,\n",
    "                    skiprows=i,\n",
    "                    dtype=test_types[file_name]\n",
    "                    )       \n",
    "\n",
    "        test_data = test_data_parts['numeric'].join(test_data_parts['categorical']).apply(LabelEncoder().fit_transform)\n",
    "        data = classifier.predict(test_data)\n",
    "        print(data)\n",
    "        print(data.transpose)\n",
    "#         data = np.append(data., test_data.index.values, axis=1)\n",
    "#         data['Id'] = test_data.index.values\n",
    "#         print(data)\n",
    "\n",
    "#         np.savetxt('data/sub/submission.csv', data, delimiter=\",\")\n",
    "        \n",
    "        \n",
    "    time_start = time.time()\n",
    "    total_start = time_start\n",
    "    \n",
    "        \n",
    "    \n",
    "#     data.to_csv('submission.csv', mode='a', header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on lines: 0 - 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/lib/arraysetops.py:200: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "<built-in method transpose of numpy.ndarray object at 0x7f2fcc638da0>\n"
     ]
    }
   ],
   "source": [
    "# train_and_save_models()\n",
    "test_and_make_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Data\n",
    "\n",
    "#### Data Exploration\n",
    "\n",
    "Overall values counts\n",
    "{0: 99432, 1: 568}\n",
    "\n",
    "#### Benchmarks\n",
    "\n",
    "Test size is 1m lines\n",
    "This will need to be divided by 3 to handle the 3 different tables\n",
    "\n",
    "Chunk Size | Time to read (seconds)\n",
    " --- | --- \n",
    "10k |271.936368942\n",
    "25k |179.683248997\n",
    "50k |163.830753088\n",
    "75k | 158.054757118\n",
    "100k |145.901106119\n",
    "150k |150.089717865\n",
    "250k |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-088e2c1ca553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bst' is not defined"
     ]
    }
   ],
   "source": [
    "bst.best_iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
